{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning and Deplying PyTorch BERT model with Amazon Elastic Inference on AWS SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification is a technique for putting text into different categories, and has a wide range of applications: email providers use text classification to detect spam emails, marketing agencies use it for sentiment analysis of customer reviews, and discussion forum moderators use it to detect inappropriate comments.\n",
    "\n",
    "In the past, data scientists used methods such as tf-idf, word2vec, or bag-of-words (BOW) to generate features for training classification models. Although these techniques have been very successful in many natural language processing (NLP) tasks, they donâ€™t always capture the meanings of words accurately when they appear in different contexts. Recently, we see increasing interest in using Bidirectional Encoder Representations from Transformers (BERT) to achieve better results in text classification tasks, due to its ability to encode the meaning of words in different contexts more accurately.\n",
    "\n",
    "Amazon SageMaker is a fully managed service that provides developers and data scientists the ability to build, train, and deploy machine learning (ML) models quickly. Amazon SageMaker removes the heavy lifting from each step of the ML process to make it easier to develop high-quality models. The Amazon SageMaker Python SDK provides open-source APIs and containers that make it easy to train and deploy models in Amazon SageMaker with several different ML and deep learning frameworks.\n",
    "\n",
    "Our customers often ask for quick fine-tuning and easy deployment of their NLP models. Furthermore, customers prefer low inference latency and low model inference cost. Amazon Elastic Inference enables attaching GPU-powered inference acceleration to endpoints, which reduces the cost of deep learning inference without sacrificing performance.\n",
    "\n",
    "This post demonstrates how to use Amazon SageMaker to fine-tune a PyTorch BERT model and deploy it with Elastic Inference. The code from this post is available in the GitHub repo. For more information about BERT fine-tuning, see BERT Fine-Tuning Tutorial with PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not going to go through the details of BERT model developed by Google. But you can access paper and look for its detail on [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT fine-tuning\n",
    "One of the biggest challenges data scientists face for NLP projects is lack of training data; you often have only a few thousand pieces of human-labeled text data for your model training. However, modern deep learning NLP tasks require a large amount of labeled data. One way to solve this problem is to use transfer learning.\n",
    "\n",
    "Transfer learning is an ML method where a pretrained model, such as a pretrained ResNet model for image classification, is reused as the starting point for a different but related problem. By reusing parameters from pretrained models, you can save significant amounts of training time and cost.\n",
    "\n",
    "BERT was trained on BookCorpus and English Wikipedia data, which contains 800 million words and 2,500 million words, respectively [1]. Training BERT from scratch would be prohibitively expensive. By taking advantage of transfer learning, you can quickly fine-tune BERT for another use case with a relatively small amount of training data to achieve state-of-the-art results for common NLP tasks, such as text classification and question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                # For matrix operations and numerical processing\n",
    "import pandas as pd                               # For munging tabular data\n",
    "import matplotlib.pyplot as plt                   # For charts and visualizations\n",
    "from IPython.display import Image                 # For displaying images in the notebook\n",
    "from IPython.display import display               # For displaying outputs in the notebook\n",
    "from time import gmtime, strftime                 # For labeling SageMaker models, endpoints, etc.\n",
    "import sys                                        # For writing outputs to notebook\n",
    "import math                                       # For ceiling function\n",
    "import json                                       # For parsing hosting outputs\n",
    "import os                                         # For manipulating filepath names\n",
    "import boto3\n",
    "import botocore\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import csv\n",
    "import io\n",
    "import re\n",
    "import s3fs\n",
    "\n",
    "\n",
    "import sagemaker                                 \n",
    "from sagemaker.predictor import csv_serializer \n",
    "from sagemaker.predictor import json_deserializer\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preparation (Specifying Sagemaker roles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sagemaker session : <sagemaker.session.Session object at 0x0000023BF7DD3508>\n",
      "S3 bucket : sagemaker-us-west-2-570447867175\n",
      "Prefix : pytorch-bert\n",
      "Region selected : us-west-2\n",
      "IAM role : arn:aws:iam::570447867175:role/SageMakerNotebookRole\n"
     ]
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()                     \n",
    "prefix = 'pytorch-bert'\n",
    "region = boto3.Session().region_name\n",
    "role = 'arn:aws:iam::570447867175:role/SageMakerNotebookRole' # pass your IAM role name\n",
    "\n",
    "print('Sagemaker session :', sagemaker_session)\n",
    "print('S3 bucket :', bucket)\n",
    "print('Prefix :', prefix)\n",
    "print('Region selected :', region)\n",
    "print('IAM role :', role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Corpus of Linguistic Acceptability (CoLA) (https://nyu-mll.github.io/CoLA/), a dataset of 10,657 English sentences labeled as grammatical or ungrammatical from published linguistics literature. We download and unzip the data using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run below for CURL\n",
    "#if not os.path.exists(\"./data/cola_public_1.1.zip\"):\n",
    "#    !curl -o ./cola_public_1.1.zip https://nyu-mll.github.io/CoLA/cola_public_1.1.zip\n",
    "#if not os.path.exists(\"./cola_public/\"):\n",
    "#    !unzip cola_public_1.1.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the training data (we use 'in_domain_train.tsv' file), the only two columns we need are the sentence and its label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"./data/in_domain_train.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    usecols=[1, 3],\n",
    "    names=[\"label\", \"sentence\"],\n",
    ")\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The professor talked us.' 'We yelled ourselves hoarse.'\n",
      " 'We yelled ourselves.' 'We yelled Harry hoarse.'\n",
      " 'Harry coughed himself into a fit.']\n",
      "[0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(sentences[20:25])\n",
    "print(labels[20:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then split the dataset for training and testing before uploading both to S3 Bucket\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df)\n",
    "train.to_csv(\"./data/CoLA_train.csv\", index=False)\n",
    "test.to_csv(\"./data/CoLA_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Add Data to S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train input path : s3://sagemaker-us-west-2-570447867175/pytorch-bert/CoLA_train.csv\n",
      "test input path : s3://sagemaker-us-west-2-570447867175/pytorch-bert/CoLA_test.csv\n"
     ]
    }
   ],
   "source": [
    "inputs_train = sagemaker_session.upload_data(\"./data/CoLA_train.csv\", bucket=bucket, key_prefix=prefix)\n",
    "inputs_test = sagemaker_session.upload_data(\"./data/CoLA_test.csv\", bucket=bucket, key_prefix=prefix)\n",
    "\n",
    "print('train input path :', inputs_train)\n",
    "print('test input path :', inputs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Start Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training script**\n",
    "We use the PyTorch-Transformers library, which contains PyTorch implementations and pre-trained model weights for many NLP models, including BERT.\n",
    "\n",
    "Our training script should save model artifacts learned during training to a file path called model_dir, as stipulated by the SageMaker PyTorch image. Upon completion of training, model artifacts saved in model_dir will be uploaded to S3 by SageMaker and will become available in S3 for deployment.\n",
    "\n",
    "We save this script in a file named train_deploy.py, and put the file in a directory named code/. The full training script can be viewed under code/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import argparse\n",
      "import json\n",
      "import logging\n",
      "import os\n",
      "import sys\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "import torch.utils.data\n",
      "import torch.utils.data.distributed\n",
      "from torch.utils.data import DataLoader, RandomSampler, TensorDataset\n",
      "from transformers import AdamW, BertForSequenceClassification, BertTokenizer\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "logger.setLevel(logging.DEBUG)\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
      "\n",
      "MAX_LEN = 64  # this is the max length of the sentence\n",
      "\n",
      "print(\"Loading BERT tokenizer...\")\n",
      "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
      "\n",
      "\n",
      "def flat_accuracy(preds, labels):\n",
      "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
      "    labels_flat = labels.flatten()\n",
      "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
      "\n",
      "\n",
      "def _get_train_data_loader(batch_size, training_dir, is_distributed):\n",
      "    logger.info(\"Get train data loader\")\n",
      "\n",
      "    dataset = pd.read_csv(os.path.join(training_dir, \"CoLA_train.csv\"))\n",
      "    sentences = dataset.sentence.values\n",
      "    labels = dataset.label.values\n",
      "\n",
      "    input_ids = []\n",
      "    for sent in sentences:\n",
      "        encoded_sent = tokenizer.encode(sent, add_special_tokens=True)\n",
      "        input_ids.append(encoded_sent)\n",
      "\n",
      "    # pad shorter sentences\n",
      "    input_ids_padded = []\n",
      "    for i in input_ids:\n",
      "        while len(i) < MAX_LEN:\n",
      "            i.append(0)\n",
      "        input_ids_padded.append(i)\n",
      "    input_ids = input_ids_padded\n",
      "\n",
      "    # mask; 0: added, 1: otherwise\n",
      "    attention_masks = []\n",
      "    # For each sentence...\n",
      "    for sent in input_ids:\n",
      "        att_mask = [int(token_id > 0) for token_id in sent]\n",
      "        attention_masks.append(att_mask)\n",
      "\n",
      "    # convert to PyTorch data types.\n",
      "    train_inputs = torch.tensor(input_ids)\n",
      "    train_labels = torch.tensor(labels)\n",
      "    train_masks = torch.tensor(attention_masks)\n",
      "\n",
      "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
      "    if is_distributed:\n",
      "        train_sampler = torch.utils.data.distributed.DistributedSampler(dataset)\n",
      "    else:\n",
      "        train_sampler = RandomSampler(train_data)\n",
      "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
      "\n",
      "    return train_dataloader\n",
      "\n",
      "\n",
      "def _get_test_data_loader(test_batch_size, training_dir):\n",
      "    dataset = pd.read_csv(os.path.join(training_dir, \"CoLA_test.csv\"))\n",
      "    sentences = dataset.sentence.values\n",
      "    labels = dataset.label.values\n",
      "\n",
      "    input_ids = []\n",
      "    for sent in sentences:\n",
      "        encoded_sent = tokenizer.encode(sent, add_special_tokens=True)\n",
      "        input_ids.append(encoded_sent)\n",
      "\n",
      "    # pad shorter sentences\n",
      "    input_ids_padded = []\n",
      "    for i in input_ids:\n",
      "        while len(i) < MAX_LEN:\n",
      "            i.append(0)\n",
      "        input_ids_padded.append(i)\n",
      "    input_ids = input_ids_padded\n",
      "\n",
      "    # mask; 0: added, 1: otherwise\n",
      "    attention_masks = []\n",
      "    # For each sentence...\n",
      "    for sent in input_ids:\n",
      "        att_mask = [int(token_id > 0) for token_id in sent]\n",
      "        attention_masks.append(att_mask)\n",
      "\n",
      "    # convert to PyTorch data types.\n",
      "    train_inputs = torch.tensor(input_ids)\n",
      "    train_labels = torch.tensor(labels)\n",
      "    train_masks = torch.tensor(attention_masks)\n",
      "\n",
      "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
      "    train_sampler = RandomSampler(train_data)\n",
      "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=test_batch_size)\n",
      "\n",
      "    return train_dataloader\n",
      "\n",
      "\n",
      "def train(args):\n",
      "    is_distributed = len(args.hosts) > 1 and args.backend is not None\n",
      "    logger.debug(\"Distributed training - %s\", is_distributed)\n",
      "    use_cuda = args.num_gpus > 0\n",
      "    logger.debug(\"Number of gpus available - %d\", args.num_gpus)\n",
      "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
      "\n",
      "    if is_distributed:\n",
      "        # Initialize the distributed environment.\n",
      "        world_size = len(args.hosts)\n",
      "        os.environ[\"WORLD_SIZE\"] = str(world_size)\n",
      "        host_rank = args.hosts.index(args.current_host)\n",
      "        os.environ[\"RANK\"] = str(host_rank)\n",
      "        dist.init_process_group(backend=args.backend, rank=host_rank, world_size=world_size)\n",
      "        logger.info(\n",
      "            \"Initialized the distributed environment: '%s' backend on %d nodes. \"\n",
      "            \"Current host rank is %d. Number of gpus: %d\",\n",
      "            args.backend, dist.get_world_size(),\n",
      "            dist.get_rank(), args.num_gpus\n",
      "        )\n",
      "\n",
      "    # set the seed for generating random numbers\n",
      "    torch.manual_seed(args.seed)\n",
      "    if use_cuda:\n",
      "        torch.cuda.manual_seed(args.seed)\n",
      "\n",
      "    train_loader = _get_train_data_loader(args.batch_size, args.data_dir, is_distributed)\n",
      "    test_loader = _get_test_data_loader(args.test_batch_size, args.test)\n",
      "\n",
      "    logger.debug(\n",
      "        \"Processes {}/{} ({:.0f}%) of train data\".format(\n",
      "            len(train_loader.sampler),\n",
      "            len(train_loader.dataset),\n",
      "            100.0 * len(train_loader.sampler) / len(train_loader.dataset),\n",
      "        )\n",
      "    )\n",
      "\n",
      "    logger.debug(\n",
      "        \"Processes {}/{} ({:.0f}%) of test data\".format(\n",
      "            len(test_loader.sampler),\n",
      "            len(test_loader.dataset),\n",
      "            100.0 * len(test_loader.sampler) / len(test_loader.dataset),\n",
      "        )\n",
      "    )\n",
      "\n",
      "    logger.info(\"Starting BertForSequenceClassification\\n\")\n",
      "    model = BertForSequenceClassification.from_pretrained(\n",
      "        \"bert-base-uncased\",  # Use the 12-layer BERT model, with an uncased vocab.\n",
      "        num_labels=args.num_labels,  # The number of output labels--2 for binary classification.\n",
      "        output_attentions=False,  # Whether the model returns attentions weights.\n",
      "        output_hidden_states=False,  # Whether the model returns all hidden-states.\n",
      "    )\n",
      "\n",
      "    model = model.to(device)\n",
      "    if is_distributed and use_cuda:\n",
      "        # multi-machine multi-gpu case\n",
      "        model = torch.nn.parallel.DistributedDataParallel(model)\n",
      "    else:\n",
      "        # single-machine multi-gpu case or single-machine or multi-machine cpu case\n",
      "        model = torch.nn.DataParallel(model)\n",
      "    optimizer = AdamW(\n",
      "        model.parameters(),\n",
      "        lr=2e-5,  # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
      "        eps=1e-8,  # args.adam_epsilon - default is 1e-8.\n",
      "    )\n",
      "\n",
      "    logger.info(\"End of defining BertForSequenceClassification\\n\")\n",
      "    for epoch in range(1, args.epochs + 1):\n",
      "        total_loss = 0\n",
      "        model.train()\n",
      "        for step, batch in enumerate(train_loader):\n",
      "            b_input_ids = batch[0].to(device)\n",
      "            b_input_mask = batch[1].to(device)\n",
      "            b_labels = batch[2].to(device)\n",
      "            model.zero_grad()\n",
      "\n",
      "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
      "            loss = outputs[0]\n",
      "\n",
      "            total_loss += loss.item()\n",
      "            loss.backward()\n",
      "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
      "            # modified based on their gradients, the learning rate, etc.\n",
      "            optimizer.step()\n",
      "            if step % args.log_interval == 0:\n",
      "                logger.info(\n",
      "                    \"Train Epoch: {} [{}/{} ({:.0f}%)] Loss: {:.6f}\".format(\n",
      "                        epoch,\n",
      "                        step * len(batch[0]),\n",
      "                        len(train_loader.sampler),\n",
      "                        100.0 * step / len(train_loader),\n",
      "                        loss.item(),\n",
      "                    )\n",
      "                )\n",
      "\n",
      "        logger.info(\"Average training loss: %f\\n\", total_loss / len(train_loader))\n",
      "\n",
      "        test(model, test_loader, device)\n",
      "\n",
      "    logger.info(\"Saving tuned model.\")\n",
      "    model_2_save = model.module if hasattr(model, \"module\") else model\n",
      "    model_2_save.save_pretrained(save_directory=args.model_dir)\n",
      "\n",
      "\n",
      "def test(model, test_loader, device):\n",
      "    model.eval()\n",
      "    _, eval_accuracy = 0, 0\n",
      "\n",
      "    with torch.no_grad():\n",
      "        for batch in test_loader:\n",
      "            b_input_ids = batch[0].to(device)\n",
      "            b_input_mask = batch[1].to(device)\n",
      "            b_labels = batch[2].to(device)\n",
      "\n",
      "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
      "            logits = outputs[0]\n",
      "            logits = logits.detach().cpu().numpy()\n",
      "            label_ids = b_labels.to(\"cpu\").numpy()\n",
      "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
      "            eval_accuracy += tmp_eval_accuracy\n",
      "\n",
      "    logger.info(\"Test set: Accuracy: %f\\n\", tmp_eval_accuracy)\n",
      "\n",
      "\n",
      "def model_fn(model_dir):\n",
      "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "    model = BertForSequenceClassification.from_pretrained(model_dir)\n",
      "    return model.to(device)\n",
      "\n",
      "\n",
      "def input_fn(request_body, request_content_type):\n",
      "    \"\"\"An input_fn that loads a pickled tensor\"\"\"\n",
      "    if request_content_type == \"application/json\":\n",
      "        sentence = json.loads(request_body)\n",
      "\n",
      "        input_ids = []\n",
      "        encoded_sent = tokenizer.encode(sentence, add_special_tokens=True)\n",
      "        input_ids.append(encoded_sent)\n",
      "\n",
      "        # pad shorter sentences\n",
      "        input_ids_padded = []\n",
      "        for i in input_ids:\n",
      "            while len(i) < MAX_LEN:\n",
      "                i.append(0)\n",
      "            input_ids_padded.append(i)\n",
      "        input_ids = input_ids_padded\n",
      "\n",
      "        # mask; 0: added, 1: otherwise\n",
      "        attention_masks = []\n",
      "        # For each sentence...\n",
      "        for sent in input_ids:\n",
      "            att_mask = [int(token_id > 0) for token_id in sent]\n",
      "            attention_masks.append(att_mask)\n",
      "\n",
      "        # convert to PyTorch data types.\n",
      "        train_inputs = torch.tensor(input_ids)\n",
      "        train_masks = torch.tensor(attention_masks)\n",
      "\n",
      "        return train_inputs, train_masks\n",
      "\n",
      "    raise ValueError(\"Unsupported content type: {}\".format(request_content_type))\n",
      "\n",
      "\n",
      "def predict_fn(input_data, model):\n",
      "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "    model.to(device)\n",
      "    model.eval()\n",
      "\n",
      "    input_id, input_mask = input_data\n",
      "    input_id = input_id.to(device)\n",
      "    input_mask = input_mask.to(device)\n",
      "    with torch.no_grad():\n",
      "        return model(input_id, token_type_ids=None, attention_mask=input_mask)[0]\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    # Data and model checkpoints directories\n",
      "    parser.add_argument(\n",
      "        \"--num_labels\", type=int, default=2, metavar=\"N\", help=\"input batch size for training (default: 64)\"\n",
      "    )\n",
      "\n",
      "    parser.add_argument(\n",
      "        \"--batch-size\", type=int, default=64, metavar=\"N\", help=\"input batch size for training (default: 64)\"\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--test-batch-size\", type=int, default=1000, metavar=\"N\", help=\"input batch size for testing (default: 1000)\"\n",
      "    )\n",
      "    parser.add_argument(\"--epochs\", type=int, default=2, metavar=\"N\", help=\"number of epochs to train (default: 10)\")\n",
      "    parser.add_argument(\"--lr\", type=float, default=0.01, metavar=\"LR\", help=\"learning rate (default: 0.01)\")\n",
      "    parser.add_argument(\"--momentum\", type=float, default=0.5, metavar=\"M\", help=\"SGD momentum (default: 0.5)\")\n",
      "    parser.add_argument(\"--seed\", type=int, default=1, metavar=\"S\", help=\"random seed (default: 1)\")\n",
      "    parser.add_argument(\n",
      "        \"--log-interval\",\n",
      "        type=int,\n",
      "        default=50,\n",
      "        metavar=\"N\",\n",
      "        help=\"how many batches to wait before logging training status\",\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--backend\",\n",
      "        type=str,\n",
      "        default=None,\n",
      "        help=\"backend for distributed training (tcp, gloo on cpu and gloo, nccl on gpu)\",\n",
      "    )\n",
      "\n",
      "    # Container environment\n",
      "    parser.add_argument(\"--hosts\", type=list, default=json.loads(os.environ[\"SM_HOSTS\"]))\n",
      "    parser.add_argument(\"--current-host\", type=str, default=os.environ[\"SM_CURRENT_HOST\"])\n",
      "    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
      "    parser.add_argument(\"--data-dir\", type=str, default=os.environ[\"SM_CHANNEL_TRAINING\"])\n",
      "    parser.add_argument(\"--test\", type=str, default=os.environ[\"SM_CHANNEL_TESTING\"])\n",
      "    parser.add_argument(\"--num-gpus\", type=int, default=os.environ[\"SM_NUM_GPUS\"])\n",
      "\n",
      "    train(parser.parse_args())\n"
     ]
    }
   ],
   "source": [
    "!pygmentize tools/train_deploy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training script should save model artifacts learned during training to a file path called model_dir, as stipulated by the Amazon SageMaker PyTorch image. Upon completion of training, Amazon SageMaker uploads model artifacts saved in model_dir to Amazon S3 so they are available for deployment. The following code is used in the script to save trained model artifacts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Amazon SageMaker to train and deploy a model using our custom PyTorch code. The Amazon SageMaker Python SDK makes it easier to run a PyTorch script in Amazon SageMaker using its PyTorch estimator. After that, we can use the SageMaker Python SDK to deploy the trained model and run predictions. For more information about using this SDK with PyTorch, see Using PyTorch with the SageMaker Python SDK.\n",
    "\n",
    "To start, we use the PyTorch estimator class to train our model. When creating the estimator, we make sure to specify the following:\n",
    "\n",
    "- entry_point â€“ The name of the PyTorch script\n",
    "- source_dir â€“ The location of the training script and requirements.txt file\n",
    "- framework_version: The PyTorch version we want to use\n",
    "\n",
    "The PyTorch estimator supports multi-machine, distributed PyTorch training. To use this, we just set train_instance_count to be greater than 1. Our training script supports distributed training for only GPU instances.\n",
    "\n",
    "After creating the estimator, we call fit(), which launches a training job. We use the Amazon S3 URIs we uploaded the training data to earlier. See the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train_deploy.py\", # the name of our PyTorch script. It contains our training script, which loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model. \n",
    "    source_dir=\"tools\", # the location of our training scripts and requirements.txt file\n",
    "    role=role,\n",
    "    framework_version=\"1.3.1\",\n",
    "    py_version=\"py3\",\n",
    "    train_instance_count=1,  # this script only support distributed training for GPU instances.\n",
    "    train_instance_type=\"ml.p2.xlarge\",\n",
    "    hyperparameters={\n",
    "        \"epochs\": 1,\n",
    "        \"num_labels\": 2,\n",
    "        \"backend\": \"gloo\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-22 05:22:22 Starting - Starting the training job...\n",
      "2020-07-22 05:22:24 Starting - Launching requested ML instances.........\n",
      "2020-07-22 05:24:22 Starting - Preparing the instances for training.........\n",
      "2020-07-22 05:25:48 Downloading - Downloading input data......\n",
      "2020-07-22 05:26:37 Training - Downloading the training image........\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-07-22 05:28:16,655 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-07-22 05:28:16,681 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-07-22 05:28:22,906 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-07-22 05:28:23,283 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-07-22 05:28:23,283 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-07-22 05:28:23,283 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-07-22 05:28:23,283 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . -r requirements.txt\u001b[0m\n",
      "\n",
      "2020-07-22 05:28:15 Training - Training image download completed. Training in progress.\u001b[34mProcessing /tmp/tmpqfnz486z/module_dir\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (4.36.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests==2.22.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (2.22.0)\u001b[0m\n",
      "\u001b[34mCollecting regex\n",
      "  Downloading regex-2020.7.14-cp36-cp36m-manylinux2010_x86_64.whl (660 kB)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\u001b[0m\n",
      "\u001b[34mCollecting transformers==2.3.0\n",
      "  Downloading transformers-2.3.0-py3-none-any.whl (447 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 2)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 2)) (2.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 2)) (1.25.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests==2.22.0->-r requirements.txt (line 2)) (2019.11.28)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from sacremoses->-r requirements.txt (line 5)) (1.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->-r requirements.txt (line 5)) (7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->-r requirements.txt (line 5)) (0.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0->-r requirements.txt (line 6)) (1.11.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from transformers==2.3.0->-r requirements.txt (line 6)) (1.16.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.15.0,>=1.14.7 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.3.0->-r requirements.txt (line 6)) (1.14.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.3.0->-r requirements.txt (line 6)) (0.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers==2.3.0->-r requirements.txt (line 6)) (0.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.15.0,>=1.14.7->boto3->transformers==2.3.0->-r requirements.txt (line 6)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.15.0,>=1.14.7->boto3->transformers==2.3.0->-r requirements.txt (line 6)) (0.15.2)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sacremoses, default-user-module-name\n",
      "  Building wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893258 sha256=b7eaeafce732971cbf19fc2ef1eb6360bcb6d8473f2e79216ba16e047388a2ca\n",
      "  Stored in directory: /root/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "  Building wheel for default-user-module-name (setup.py): started\n",
      "  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=116850 sha256=e9af9f9faefe8f2c111e0ebf1b1d95ff49cbdf227828591fa47b227b5149b952\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-b5apoeh3/wheels/1d/04/73/0feb4a606059cfc8fd30b0632e20d841488823236faf7bc0ac\u001b[0m\n",
      "\u001b[34mSuccessfully built sacremoses default-user-module-name\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, sentencepiece, sacremoses, transformers, default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0 regex-2020.7.14 sacremoses-0.0.43 sentencepiece-0.1.91 transformers-2.3.0\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 20.0.1; however, version 20.1.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-07-22 05:28:28,627 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"testing\": \"/opt/ml/input/data/testing\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": 1,\n",
      "        \"num_labels\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"testing\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-07-22-05-22-19-755\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-570447867175/pytorch-training-2020-07-22-05-22-19-755/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_deploy\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_deploy.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"backend\":\"gloo\",\"epochs\":1,\"num_labels\":2}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_deploy.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"testing\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_deploy\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-570447867175/pytorch-training-2020-07-22-05-22-19-755/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"testing\":\"/opt/ml/input/data/testing\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"backend\":\"gloo\",\"epochs\":1,\"num_labels\":2},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"testing\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-07-22-05-22-19-755\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-570447867175/pytorch-training-2020-07-22-05-22-19-755/source/sourcedir.tar.gz\",\"module_name\":\"train_deploy\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_deploy.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--epochs\",\"1\",\"--num_labels\",\"2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TESTING=/opt/ml/input/data/testing\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_LABELS=2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train_deploy.py --backend gloo --epochs 1 --num_labels 2\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mLoading BERT tokenizer...\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [0/6413 (0%)] Loss: 0.607301\u001b[0m\n",
      "\u001b[34mDistributed training - False\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [3200/6413 (50%)] Loss: 0.543899\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 1\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [1300/6413 (99%)] Loss: 0.583049\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34mINFO:__main__:Average training loss: 0.516400\u001b[0m\n",
      "\u001b[34mProcesses 6413/6413 (100%) of train data\n",
      "\u001b[0m\n",
      "\u001b[34mProcesses 2138/2138 (100%) of test data\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Accuracy: 0.804348\u001b[0m\n",
      "\u001b[34mStarting BertForSequenceClassification\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving tuned model.\u001b[0m\n",
      "\u001b[34mEnd of defining BertForSequenceClassification\u001b[0m\n",
      "\u001b[34mINFO:transformers.configuration_utils:Configuration saved in /opt/ml/model/config.json\n",
      "\u001b[0m\n",
      "\u001b[34mINFO:transformers.modeling_utils:Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:04.689 algo-1:48 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:04.689 algo-1:48 INFO hook.py:152] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:04.689 algo-1:48 INFO hook.py:197] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:04.710 algo-1:48 INFO hook.py:326] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.402 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.402 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.402 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.403 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.410 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.410 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.410 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.0 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.413 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.413 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.413 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.414 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.416 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.416 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.416 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.1 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.419 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.419 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.419 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.420 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.422 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.422 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.422 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.2 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.426 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.426 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.426 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.426 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.429 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.429 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.429 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.3 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.432 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.432 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.432 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.433 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.435 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.435 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.435 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.4 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.438 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.438 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.438 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.439 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.441 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.441 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.441 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.5 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.444 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.444 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.444 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.445 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.447 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.447 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.447 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.6 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.451 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.451 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.451 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.451 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.454 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.454 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.454 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.7 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.457 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.457 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.457 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.458 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.462 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.462 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.462 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.8 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.467 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.467 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.467 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.468 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.472 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.472 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.472 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.9 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.477 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.477 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.477 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.478 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.482 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.482 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.482 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.10 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.487 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.487 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.487 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention.self NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.488 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11.attention NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.492 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.492 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:29:05.492 algo-1:48 WARNING hook.py:808] var is not Tensor or list or tuple of Tensors, module_name:module.bert.encoder.layer.11 NoneType\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [0/6413 (0%)] Loss: 0.607301\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [3200/6413 (50%)] Loss: 0.543899\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [1300/6413 (99%)] Loss: 0.583049\u001b[0m\n",
      "\u001b[34mAverage training loss: 0.516400\n",
      "\u001b[0m\n",
      "\u001b[34mTest set: Accuracy: 0.804348\n",
      "\u001b[0m\n",
      "\u001b[34mSaving tuned model.\u001b[0m\n",
      "\u001b[34m[2020-07-22 05:31:37.774 algo-1:48 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2020-07-22 05:31:38,195 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-07-22 05:32:22 Uploading - Uploading generated training model\n",
      "2020-07-22 05:33:26 Completed - Training job completed\n",
      "Training seconds: 458\n",
      "Billable seconds: 458\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({\"training\": inputs_train, \"testing\": inputs_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training our model, we host it on an Amazon SageMaker endpoint by calling deploy on the PyTorch estimator. The endpoint runs an Amazon SageMaker PyTorch model server. We need to configure two components of the server: model loading and model serving. We implement these two components in our inference script train_deploy.py. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'model_fn()' is the function defined to load the saved model and return a model object that can be used for model serving. The SageMaker PyTorch model server loads our model by invoking model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(model_dir):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'input_fn()' deserializes and prepares the prediction input. In this use case, our request body is first serialized to JSON and then sent to model serving endpoint. Therefore, in input_fn(), we first deserialize the JSON-formatted request body and return the input as a torch.tensor, as required for BERT: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(request_body, request_content_type):\n",
    "    if request_content_type == \"application/json\":\n",
    "        sentence = json.loads(request_body)\n",
    "        \n",
    "        input_ids = []\n",
    "        encoded_sent = tokenizer.encode(sentence,add_special_tokens = True)\n",
    "        input_ids.append(encoded_sent)\n",
    "    \n",
    "        # pad shorter sentences\n",
    "        input_ids_padded =[]\n",
    "        for i in input_ids:\n",
    "            while len(i) < MAX_LEN:\n",
    "                i.append(0)\n",
    "            input_ids_padded.append(i)\n",
    "        input_ids = input_ids_padded\n",
    "    \n",
    "        # mask; 0: added, 1: otherwise\n",
    "        [int(token_id > 0) for token_id in sent] for sent in input_ids\n",
    "\n",
    "        # convert to PyTorch data types.\n",
    "        train_inputs = torch.tensor(input_ids)\n",
    "        train_masks = torch.tensor(attention_masks)\n",
    "    \n",
    "        # train_data = TensorDataset(train_inputs, train_masks)\n",
    "        return train_inputs, train_masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict_fn() performs the prediction and returns the result. See the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fn(input_data, model):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    input_id, input_mask = input_data\n",
    "    input_id.to(device)\n",
    "    input_mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        return model(input_id, token_type_ids=None,attention_mask=input_mask)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deploy the model with deploy() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type=\"ml.p2.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then configure the predictor to use application/json for the content type when sending requests to our endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import json_deserializer, json_serializer\n",
    "\n",
    "predictor.content_type = \"application/json\"\n",
    "predictor.accept = \"application/json\"\n",
    "predictor.serializer = json_serializer\n",
    "predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the returned predictor object to call the endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "result = predictor.predict(\"Somebody just left - guess who.\")\n",
    "print(np.argmax(result, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. (Optional) Use a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to reuse pretrained model, you can create a PyTorchModel from existing model artifacts in S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel \n",
    "\n",
    "pytorch_model = PyTorchModel(model_data=\"<S3 location>/model.tar.gz\",\n",
    "                             role=role,\n",
    "                             framework_version=\"1.3.1\",\n",
    "                             source_dir=\"tools\",\n",
    "                             entry_point=\"train_deploy.py\")\n",
    "\n",
    "predictor = pytorch_model.deploy(initial_instance_count=1, instance_type=\"ml.p2.xlarge\")\n",
    "\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Deploy the model (endpoint) with Elastic Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the right instance type for inference requires deciding between different amounts of GPU, CPU, and memory resources. Optimizing for one of these resources on a standalone GPU instance usually leads to underutilization of other resources. Elastic Inference solves this problem by enabling you to attach the right amount of GPU-powered inference acceleration to your endpoint. In March 2020, Elastic Inference support for PyTorch became available for both Amazon SageMaker and Amazon EC2.\n",
    "[Link](https://aws.amazon.com/ko/blogs/machine-learning/reduce-ml-inference-costs-on-amazon-sagemaker-for-pytorch-models-using-amazon-elastic-inference/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Elastic Inference, we must first convert our trained model to TorchScript. For more information, see Reduce ML inference costs on Amazon SageMaker for PyTorch models using Amazon Elastic Inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-west-2-570447867175/pytorch-training-2020-07-22-05-22-19-755/output/model.tar.gz'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a folder to save model trained model, and download the model.tar.gz file to local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "%%sh -s $estimator.model_data\n",
    "mkdir model\n",
    "aws s3 cp $1 model/ \n",
    "tar xvzf model/model.tar.gz --directory ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the trained model artifacts from Amazon S3. The location of the model artifacts is estimator.model_data. We then convert the model to TorchScript using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0721 23:19:17.148522 24952 configuration_utils.py:283] loading configuration file ./model/config.json\n",
      "I0721 23:19:17.150515 24952 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"torchscript\": true,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "I0721 23:19:17.152509 24952 modeling_utils.py:615] loading weights file ./model/pytorch_model.bin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model_torchScript = BertForSequenceClassification.from_pretrained(\"./model/\", torchscript=True)\n",
    "device = \"cpu\"\n",
    "for_jit_trace_input_ids = [0] * 64\n",
    "for_jit_trace_attention_masks = [0] * 64\n",
    "for_jit_trace_input = torch.tensor([for_jit_trace_input_ids])\n",
    "for_jit_trace_masks = torch.tensor([for_jit_trace_input_ids])\n",
    "\n",
    "traced_model = torch.jit.trace(\n",
    "    model_torchScript, [for_jit_trace_input.to(device), for_jit_trace_masks.to(device)]\n",
    ")\n",
    "torch.jit.save(traced_model, \"traced_bert.pt\")\n",
    "\n",
    "subprocess.call([\"tar\", \"-czvf\", \"traced_bert.tar.gz\", \"traced_bert.pt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the TorchScript model and using it for prediction require small changes in our model loading and prediction functions. We create a new script deploy_ei.py that is slightly different from train_deploy.py script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import json\n",
      "import logging\n",
      "import os\n",
      "import sys\n",
      "\n",
      "import torch\n",
      "import torch.utils.data\n",
      "import torch.utils.data.distributed\n",
      "from transformers import BertTokenizer\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "logger.setLevel(logging.DEBUG)\n",
      "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
      "\n",
      "MAX_LEN = 64  # this is the max length of the sentence\n",
      "\n",
      "print(\"Loading BERT tokenizer...\")\n",
      "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
      "\n",
      "\n",
      "def model_fn(model_dir):\n",
      "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "\n",
      "    loaded_model = torch.jit.load(os.path.join(model_dir, \"traced_bert.pt\"))\n",
      "    return loaded_model.to(device)\n",
      "\n",
      "\n",
      "def input_fn(request_body, request_content_type):\n",
      "    \"\"\"An input_fn that loads a pickled tensor\"\"\"\n",
      "    if request_content_type == \"application/json\":\n",
      "        sentence = json.loads(request_body)\n",
      "\n",
      "        input_ids = []\n",
      "        encoded_sent = tokenizer.encode(sentence, add_special_tokens=True)\n",
      "        input_ids.append(encoded_sent)\n",
      "\n",
      "        # pad shorter sentences\n",
      "        input_ids_padded = []\n",
      "        for i in input_ids:\n",
      "            while len(i) < MAX_LEN:\n",
      "                i.append(0)\n",
      "            input_ids_padded.append(i)\n",
      "        input_ids = input_ids_padded\n",
      "\n",
      "        # mask; 0: added, 1: otherwise\n",
      "        attention_masks = []\n",
      "        # For each sentence...\n",
      "        for sent in input_ids:\n",
      "            att_mask = [int(token_id > 0) for token_id in sent]\n",
      "            attention_masks.append(att_mask)\n",
      "\n",
      "        # convert to PyTorch data types.\n",
      "        train_inputs = torch.tensor(input_ids)\n",
      "        train_masks = torch.tensor(attention_masks)\n",
      "\n",
      "        return train_inputs, train_masks\n",
      "\n",
      "    raise ValueError(\"Unsupported content type: {}\".format(request_content_type))\n",
      "\n",
      "\n",
      "def predict_fn(input_data, model):\n",
      "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "    model.to(device)\n",
      "    model.eval()\n",
      "\n",
      "    input_id, input_mask = input_data\n",
      "    input_id = input_id.to(device)\n",
      "    input_mask = input_mask.to(device)\n",
      "    with torch.no_grad():\n",
      "        with torch.jit.optimized_execution(True, {\"target_device\": \"eia:0\"}):\n",
      "            return model(input_id, attention_mask=input_mask)[0]\n"
     ]
    }
   ],
   "source": [
    "!pygmentize tools/deploy_ei.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we upload TorchScript model to S3 and deploy using Elastic Inference. The accelerator_type=ml.eia2.xlarge parameter is how we attach the Elastic Inference accelerator to our endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload tarball to S3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0721 23:26:41.529985 24952 model.py:111] Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint name : bert-ei-traced-mlp2large-mleia2xlarge\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "\n",
    "instance_type = 'ml.p2.large'\n",
    "accelerator_type = 'ml.eia2.xlarge'\n",
    "\n",
    "# TorchScript model\n",
    "tar_filename = 'traced_bert.tar.gz'\n",
    "\n",
    "# Returns S3 bucket URL\n",
    "print('Upload tarball to S3')\n",
    "model_data = sagemaker_session.upload_data(path=tar_filename, bucket=bucket, key_prefix=prefix)\n",
    "\n",
    "endpoint_name = 'bert-ei-traced-{}-{}'.format(instance_type, accelerator_type).replace('.', '').replace('_', '')\n",
    "print('endpoint name :', endpoint_name)\n",
    "\n",
    "pytorch = PyTorchModel(\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    entry_point='deploy_ei.py',\n",
    "    source_dir='tools',\n",
    "    framework_version='1.3.1',\n",
    "    py_version='py3',\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function will exit before endpoint is finished creating\n",
    "predictor = pytorch.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Close the SageMaker Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(endpoint_name)\n",
    "predictor.delete_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, we used Amazon SageMaker to take BERT as a starting point and train a model for labeling sentences on their grammatical completeness. We then deployed the model to an Amazon SageMaker endpoint, both with and without Elastic Inference acceleration. You can use this solution to tune BERT in other ways, or use other pretrained models provided by PyTorch-Transformers. For more about using PyTorch with Amazon SageMaker, see Using PyTorch with the SageMaker Python SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    ">https://aws.amazon.com/ko/blogs/machine-learning/fine-tuning-a-pytorch-bert-model-and-deploying-it-with-amazon-elastic-inference-on-amazon-sagemaker/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
